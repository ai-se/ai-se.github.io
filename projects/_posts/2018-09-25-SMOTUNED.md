---
title:  Is better data better than better data miners?
layout: page

excerpt: Tune parameters, don't use off-the-shelf parameters!
---

with _[Amritanshu Agrawal](/people/2014/05/17/Amritanshu-Agrawal/),_ NC State
     
<img align="left" width="500"
 src="/img/smote.png"> 
     
We report and fix an important systematic error in prior studies
that ranked classifiers for software analytics. Those studies did not
(a) assess classifiers on multiple criteria and they did not (b) study
how variations in the data affect the results. Hence, this paper
applies (a) multi-performance criteria while (b) fixing the weaker
regions of the training data (using SMOTUNED, which is an autotuning
version of SMOTE). This approach leads to dramatically
large increases in software defect predictions when applied in a
5*5 cross-validation study for 3,681 JAVA classes (containing over
a million lines of code) from open source systems, SMOTUNED
increased AUC and recall by 60% and 20% respectively. These improvements
are independent of the classifier used to predict for
defects. Same kind of pattern (improvement) was observed when a
comparative analysis of SMOTE and SMOTUNED was done against
the most recent class imbalance technique.

In conclusion, for software analytic tasks like defect prediction,
(1) data pre-processing can be more important than classifier choice,
(2) ranking studies are incomplete without such pre-processing,
and (3) SMOTUNED is a promising candidate for pre-processing. _[Read More](https://dl.acm.org/citation.cfm?id=3180197)_

_[Python Tool](https://github.com/ai-se/Smote_tune/tree/master/fault_prediction)_
