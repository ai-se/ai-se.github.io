---
title: Hyperparameter tuning on effort estimation
layout: page

excerpt: Hyperparameter tuning makes performance better
---

with _Tianpei Xia,_ NC State University

<img align="left" width="300"
 src="/img/feature_model_tree.png">     

SE researchers use search-based optimization techniques to solve SE problems. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. Meanwhile, software analytics has been widely used in software engineering for many tasks such as estimating software project effort. One of the ``black arts'' of software analytics is tuning the parameters controlling predicting algorithms. Such {\em hyperparameter optimization} has been widely studied in some software analytics domains (e.g. defect prediction and text mining). But, so far, has not been extensively explored for effort estimation. Accordingly, this report seeks simple, automatic, effective and fast methods for finding hyperparameter options for automatic software effort estimation.

We introduce a hyperparameter optimization architecture called RATE (\underline{R}apid \underline{A}utomatic \underline{T}uning for \underline{E}ffort-estimation). We test FATE on a wide range of  hyperparameter optimizers using data from near a thousand software projects. After tuning, large improvements in effort estimation accuracy were observed (measured in terms of the magnitude of the relative error and standardized accuracy).
  
From those results, we recommend using regression trees (CART) tuned by either different evolution or Flash. This particular combination of learner and optimizers often achieves
in one or two hours what other optimizers need days to weeks of CPU time to accomplish.

Full paper can be accessed [here](https://arxiv.org/pdf/1805.00336.pdf)
